- 2019-12-05
  
  - testmodel v24 epoch 10, hd#113/114,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.0004
    
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.066319 860.353943 0.138767 0.064399 0.029512
    r(raw): 0.219934 376.352692 0.365973 0.240399 0.145852
    averaged(raw): 0.143126 618.353333 0.252370 0.152399 0.087682
    
    l(filter): 0.092338 559.318542 0.180250 0.093619 0.046076
    r(filter): 0.274104 354.540405 0.430519 0.307046 0.191342
    averaged(filter): 0.183221 456.929474 0.305385 0.200332 0.118709
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.121578 281.728577 0.240887 0.123962 0.063178
    r(raw): 0.243817 175.082275 0.395632 0.263461 0.166178
    averaged(raw): 0.182697 228.405426 0.318260 0.193712 0.114678
    
    l(filter): 0.230701 131.228226 0.380680 0.246897 0.154305
    r(filter): 0.328653 153.270004 0.496531 0.359963 0.242891
    averaged(filter): 0.279677 142.249115 0.438605 0.303430 0.198598
    ```
    
    
    
  - testmodel v25 epoch 10, hd#115/116,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.0002
    
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.071073 947.626526 0.142334 0.069677 0.034594
    r(raw): 0.223613 424.096161 0.371152 0.240106 0.150396
    averaged(raw): 0.147343 685.861328 0.256743 0.154891 0.092495
    
    l(filter): 0.102557 667.491333 0.181423 0.103733 0.059660
    r(filter): 0.285929 402.506683 0.445226 0.319506 0.203069
    averaged(filter): 0.194243 534.999023 0.313325 0.211619 0.131364
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.125088 279.999084 0.243282 0.128750 0.066403
    r(raw): 0.248414 183.837097 0.401397 0.263559 0.171895
    averaged(raw): 0.186751 231.918091 0.322339 0.196155 0.119149
    
    l(filter): 0.241350 139.948303 0.386495 0.256621 0.168035
    r(filter): 0.338898 162.247635 0.506743 0.371641 0.253103
    averaged(filter): 0.290124 151.097961 0.446619 0.314131 0.210569
    ```
    
    
    
  - testmodel v27 epoch 10, hd#117/118,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.0001
    
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.066968 1208.106689 0.129630 0.066940 0.034301
    r(raw): 0.227637 599.516663 0.363432 0.243184 0.160510
    averaged(raw): 0.147302 903.811646 0.246531 0.155062 0.097405
    
    l(filter): 0.093391 946.791565 0.158800 0.095133 0.057119
    r(filter): 0.294098 578.278748 0.429933 0.324001 0.221978
    averaged(filter): 0.193744 762.535156 0.294366 0.209567 0.139549
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.120710 292.484863 0.234096 0.124597 0.064302
    r(raw): 0.249191 199.497696 0.398173 0.264487 0.175462
    averaged(raw): 0.184950 245.991272 0.316134 0.194542 0.119882
    
    l(filter): 0.237287 161.841690 0.375745 0.250366 0.167595
    r(filter): 0.340122 178.259796 0.496922 0.371299 0.259113
    averaged(filter): 0.288704 170.050751 0.436333 0.310833 0.213354
    ```
    
    
    
  - ~~testmodel v21 epoch 10, hd#107/108,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.008~~
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    r(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    averaged(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    
    l(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    r(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    averaged(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    r(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    averaged(raw): 1.000000 1.000000 1.000000 1.000000 1.000000
    
    l(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    r(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    averaged(filter): 1.000000 1.000000 1.000000 1.000000 1.000000
    ```
  
    
  
  - testmodel v22 epoch 10, hd#109/110,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.0008
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.065179 833.360291 0.139011 0.060588 0.027704
    r(raw): 0.202368 335.001526 0.358497 0.222271 0.124548
    averaged(raw): 0.133774 584.180908 0.248754 0.141430 0.076126
    
    l(filter): 0.092884 518.313477 0.186211 0.092104 0.045197
    r(filter): 0.253280 312.728027 0.418548 0.287892 0.166031
    averaged(filter): 0.173082 415.520752 0.302380 0.189998 0.105614
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.117685 277.282532 0.238835 0.120590 0.058634
    r(raw): 0.234985 164.346725 0.393628 0.255692 0.153572
    averaged(raw): 0.176335 220.814636 0.316232 0.188141 0.106103
    
    l(filter): 0.225317 119.771965 0.383905 0.240154 0.147269
    r(filter): 0.309177 142.073532 0.487687 0.346868 0.215479
    averaged(filter): 0.267247 130.922745 0.435796 0.293511 0.181374
    ```
  
    
  
  - testmodel v23 epoch 10, hd#111/112,  ent_dimension = 100, rel_dimension = 50 (as v17) , alpha = 0.0006
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.067867 826.917908 0.141259 0.066061 0.029757
    r(raw): 0.210832 345.132263 0.360696 0.231652 0.134320
    averaged(raw): 0.139350 586.025085 0.250977 0.148857 0.082038
    
    l(filter): 0.094606 521.202942 0.185674 0.097039 0.046663
    r(filter): 0.261323 323.189423 0.417815 0.292534 0.178638
    averaged(filter): 0.177964 422.196167 0.301744 0.194786 0.112650
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.120833 276.203369 0.241180 0.123669 0.061517
    r(raw): 0.240498 168.495499 0.392847 0.258282 0.162465
    averaged(raw): 0.180665 222.349426 0.317014 0.190975 0.111991
    
    l(filter): 0.228327 123.361038 0.382537 0.242940 0.150933
    r(filter): 0.317539 146.552765 0.487101 0.346575 0.231017
    averaged(filter): 0.272933 134.956909 0.434819 0.294757 0.190975
    ```
  
    
  
- 2019-12-04
  
  - testmodel v18 epoch 5, hd#101/102,  ent_dimension = 100, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.056256 943.303162 0.119466 0.053699 0.022867
    r(raw): 0.206434 415.246307 0.344474 0.221831 0.137350
    averaged(raw): 0.131345 679.274719 0.231970 0.137765 0.080108
    
    l(filter): 0.075080 652.788818 0.152350 0.075735 0.033226
    r(filter): 0.250124 393.899353 0.393433 0.276214 0.175511
    averaged(filter): 0.162602 523.344116 0.272892 0.175975 0.104368
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.107322 292.444946 0.219975 0.106372 0.052428
    r(raw): 0.230283 191.270798 0.376869 0.246653 0.155526
    averaged(raw): 0.168802 241.857880 0.298422 0.176512 0.103977
    
    l(filter): 0.204273 147.194427 0.351461 0.215479 0.132561
    r(filter): 0.300068 169.923828 0.463891 0.331770 0.215382
    averaged(filter): 0.252170 158.559128 0.407676 0.273625 0.173971
    ```
  
    
  
  - testmodel v19 epoch 10, hd#103/104,  ent_dimension = 100, rel_dimension = 200
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.060986 845.530945 0.122447 0.057657 0.026581
    r(raw): 0.190142 343.933014 0.338171 0.212157 0.112137
    averaged(raw): 0.125564 594.731995 0.230309 0.134907 0.069359
    
    l(filter): 0.081672 525.305298 0.166520 0.079156 0.036597
    r(filter): 0.234497 321.630219 0.391283 0.266051 0.149614
    averaged(filter): 0.158084 423.467773 0.278902 0.172603 0.093106
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.106497 285.894562 0.221392 0.105150 0.050767
    r(raw): 0.221709 177.369644 0.381364 0.244699 0.138278
    averaged(raw): 0.164103 231.632111 0.301378 0.174924 0.094523
    
    l(filter): 0.198307 125.789017 0.354686 0.207319 0.122252
    r(filter): 0.287696 155.067825 0.465357 0.322975 0.193638
    averaged(filter): 0.243001 140.428421 0.410021 0.265147 0.157945
    ```
  
    
  
  - testmodel v20 epoch 10, hd#105/106,  ent_dimension = 50, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.058951 835.722046 0.127431 0.056679 0.022769
    r(raw): 0.206322 357.921051 0.353025 0.225789 0.131438
    averaged(raw): 0.132637 596.821533 0.240228 0.141234 0.077103
    
    l(filter): 0.079389 527.111023 0.163979 0.078911 0.033861
    r(filter): 0.251127 336.247131 0.409850 0.281736 0.168621
    averaged(filter): 0.165258 431.679077 0.286915 0.180323 0.101241
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.108497 286.870972 0.223688 0.107495 0.052135
    r(raw): 0.231181 184.797134 0.382195 0.246506 0.155086
    averaged(raw): 0.169839 235.834045 0.302941 0.177001 0.103611
    
    l(filter): 0.206875 132.578186 0.357569 0.218167 0.132659
    r(filter): 0.294778 163.123718 0.466383 0.324783 0.207710
    averaged(filter): 0.250827 147.850952 0.411976 0.271475 0.170185
    ```
  
    
  
- 2019-12-03
  
  |                                                              | MRR      | MR         | hit@10   |
  | ------------------------------------------------------------ | -------- | ---------- | -------- |
  | Epoch 30 hd#83,84, testmodel v8                              | 0.158171 | 373.906738 | 0.306802 |
  | Epoch 40 hd#85,86, testmodel v9                              | 0.145764 | 380.769836 | 0.297347 |
  | Epoch 60 testmodel v7                                        | 0.142836 | 405.539612 | 0.296809 |
  | testmodel v12 epoch 60, hd#89/90,  ent_dimension = 100, rel_dimension = 50 | 0.141229 | 393.915283 | 0.298422 |
  | testmodel v13 epoch 30, hd#91/92,  ent_dimension = 100, rel_dimension = 50 | 0.156903 | 364.584229 | 0.313838 |
  | testmodel v14 epoch 20, hd#93/94,  ent_dimension = 100, rel_dimension = 50 | 0.163280 | 370.778656 | 0.313496 |
  | testmodel v15 epoch 30, hd#95/96,  ent_dimension = 200, rel_dimension = 50 | 0.142727 | 401.151367 | 0.289895 |
  | testmodel v16 epoch 30, hd#97/98,  ent_dimension = 300, rel_dimension = 50 | 0.124340 | 439.743408 | 0.264585 |
  | testmodel v17 epoch 10, hd#99/100,  ent_dimension = 100, rel_dimension = 50 | 0.167178 | 404.371826 | 0.299252 |
  
  - testmodel v17 epoch 10, hd#99/100,  ent_dimension = 100, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.063426 816.078186 0.139549 0.060344 0.025017
    r(raw): 0.190926 331.812714 0.344474 0.211668 0.112039
    averaged(raw): 0.127176 573.945435 0.242011 0.136006 0.068528
    
    l(filter): 0.090153 499.516113 0.186846 0.089417 0.040213
    r(filter): 0.244204 309.227509 0.411658 0.274993 0.158702
    averaged(filter): 0.167178 404.371826 0.299252 0.182205 0.099458
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.115911 274.349792 0.239959 0.115606 0.056924
    r(raw): 0.231976 165.965607 0.388205 0.255741 0.148490
    averaged(raw): 0.173944 220.157700 0.314082 0.185674 0.102707
    
    l(filter): 0.221944 116.082237 0.386446 0.234926 0.142431
    r(filter): 0.306203 143.380539 0.484218 0.340711 0.214209
    averaged(filter): 0.264074 129.731384 0.435332 0.287819 0.178320
    ```
  
    
  
  - testmodel v16 epoch 30, hd#97/98,  ent_dimension = 300, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.044881 923.837402 0.105834 0.036548 0.011580
    r(raw): 0.112405 346.613312 0.252174 0.111795 0.044659
    averaged(raw): 0.078643 635.225342 0.179004 0.074172 0.028120
    
    l(filter): 0.082194 559.419006 0.185723 0.080524 0.029952
    r(filter): 0.166486 320.067780 0.343448 0.179420 0.083162
    averaged(filter): 0.124340 439.743408 0.264585 0.129972 0.056557
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.108164 284.324921 0.242500 0.106958 0.046467
    r(raw): 0.203250 139.261612 0.365435 0.218362 0.120932
    averaged(raw): 0.155707 211.793274 0.303968 0.162660 0.083700
    
    l(filter): 0.231469 102.129242 0.410583 0.249487 0.144826
    r(filter): 0.294093 112.716553 0.487833 0.327714 0.197401
    averaged(filter): 0.262781 107.422897 0.449208 0.288601 0.171113
    ```
  
    
  
  - testmodel v15 epoch 30, hd#95/96,  ent_dimension = 200, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.052024 880.868958 0.118098 0.044611 0.016662
    r(raw): 0.135337 310.425293 0.290482 0.140330 0.061028
    averaged(raw): 0.093681 595.647095 0.204290 0.092470 0.038845
    
    l(filter): 0.088983 518.046082 0.196668 0.088879 0.034545
    r(filter): 0.196471 284.256622 0.383123 0.216505 0.107398
    averaged(filter): 0.142727 401.151367 0.289895 0.152692 0.070971
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.113531 279.965637 0.246946 0.114580 0.050914
    r(raw): 0.210621 136.371353 0.377455 0.229014 0.125623
    averaged(raw): 0.162076 208.168488 0.312201 0.171797 0.088268
    
    l(filter): 0.235278 98.561958 0.414199 0.252516 0.148002
    r(filter): 0.298460 110.203117 0.496824 0.335532 0.197547
    averaged(filter): 0.266869 104.382538 0.455512 0.294024 0.172774
    ```
  
    
  
  - testmodel v14 epoch 20, hd#93/94,  ent_dimension = 100, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.062168 821.331604 0.136373 0.058829 0.023160
    r(raw): 0.170331 289.929260 0.334311 0.190071 0.087071
    averaged(raw): 0.116249 555.630432 0.235342 0.124450 0.055116
    
    l(filter): 0.096820 475.612488 0.209078 0.097332 0.041093
    r(filter): 0.229740 265.944824 0.417913 0.261751 0.135933
    averaged(filter): 0.163280 370.778656 0.313496 0.179542 0.088513
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.116787 273.806793 0.246946 0.119760 0.054285
    r(raw): 0.222554 146.187241 0.385420 0.247386 0.134418
    averaged(raw): 0.169670 209.997009 0.316183 0.183573 0.094352
    
    l(filter): 0.230430 100.959839 0.406821 0.247142 0.144923
    r(filter): 0.303790 122.203751 0.495407 0.343106 0.203362
    averaged(filter): 0.267110 111.581795 0.451114 0.295124 0.174142
    ```
  
    
  
  - testmodel v13 epoch 30, hd#91/92,  ent_dimension = 100, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.054002 827.369873 0.125086 0.048080 0.016075
    r(raw): 0.162060 284.357758 0.335776 0.171309 0.083260
    averaged(raw): 0.108031 555.863831 0.230431 0.109694 0.049668
    
    l(filter): 0.090416 469.850922 0.203264 0.089417 0.034643
    r(filter): 0.223390 259.317505 0.424411 0.248656 0.130411
    averaged(filter): 0.156903 364.584229 0.313838 0.169036 0.082527
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.114235 273.865387 0.250318 0.116437 0.049936
    r(raw): 0.214167 136.819748 0.388351 0.234780 0.128897
    averaged(raw): 0.164201 205.342560 0.319335 0.175608 0.089417
    
    l(filter): 0.232047 95.116440 0.412342 0.246506 0.146047
    r(filter): 0.299081 111.780022 0.502052 0.339881 0.197498
    averaged(filter): 0.265564 103.448227 0.457197 0.293194 0.171773
    ```
  
    
  
  - testmodel v12 epoch 60, hd#89/90,  ent_dimension = 100, rel_dimension = 50
  
    ```log
    no type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.055600 	 878.755188 	 0.124255 	 0.049546 	 0.019007 
    r(raw):			 0.130364 	 308.199005 	 0.292778 	 0.137350 	 0.052624 
    averaged(raw):		 0.092982 	 593.477112 	 0.208517 	 0.093448 	 0.035815 
    
    l(filter):		 0.096469 	 506.357666 	 0.204974 	 0.097430 	 0.041874 
    r(filter):		 0.185989 	 281.472870 	 0.391869 	 0.210495 	 0.090101 
    averaged(filter):	 0.141229 	 393.915283 	 0.298422 	 0.153963 	 0.065987 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.114052 	 281.670776 	 0.246995 	 0.115313 	 0.051353 
    r(raw):			 0.200156 	 133.419617 	 0.368660 	 0.212059 	 0.117268 
    averaged(raw):		 0.157104 	 207.545197 	 0.307828 	 0.163686 	 0.084311 
    
    l(filter):		 0.235684 	 95.483482 	 0.412049 	 0.250953 	 0.150347 
    r(filter):		 0.282283 	 106.693787 	 0.492573 	 0.315010 	 0.179517 
    averaged(filter):	 0.258983 	 101.088638 	 0.452311 	 0.282982 	 0.164932 
    ```
  
  - Tried concatenate the vector entity and relation.
  
  - Epoch 60 testmodel v7
  
    ```log
    no type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.051999 	 899.403320 	 0.122056 	 0.044562 	 0.015978 
    r(raw):			 0.140901 	 312.573059 	 0.305336 	 0.144581 	 0.064399 
    averaged(raw):		 0.096450 	 605.988159 	 0.213696 	 0.094571 	 0.040189 
    
    l(filter):		 0.088063 	 525.484314 	 0.197156 	 0.086289 	 0.034887 
    r(filter):		 0.197609 	 285.594879 	 0.396462 	 0.220805 	 0.103538 
    averaged(filter):	 0.142836 	 405.539612 	 0.296809 	 0.153547 	 0.069212 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.109704 	 285.339294 	 0.244943 	 0.110574 	 0.046467 
    r(raw):			 0.204109 	 133.876038 	 0.363383 	 0.213623 	 0.125770 
    averaged(raw):		 0.156907 	 209.607666 	 0.304163 	 0.162098 	 0.086118 
    
    l(filter):		 0.225282 	 98.394852 	 0.406088 	 0.241865 	 0.137447 
    r(filter):		 0.285529 	 106.898026 	 0.484218 	 0.318382 	 0.188117 
    averaged(filter):	 0.255406 	 102.646439 	 0.445153 	 0.280123 	 0.162782 
    ```
  
  - Epoch 40 hd#85,86, testmodel v9
  
    ```log
    no type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.055440 	 856.792847 	 0.121519 	 0.051158 	 0.018567 
    r(raw):			 0.149082 	 294.276306 	 0.311737 	 0.154060 	 0.073585 
    averaged(raw):		 0.102261 	 575.534546 	 0.216628 	 0.102609 	 0.046076 
    
    l(filter):		 0.089220 	 492.765869 	 0.194078 	 0.089612 	 0.035131 
    r(filter):		 0.202308 	 268.773773 	 0.400616 	 0.222711 	 0.109450 
    averaged(filter):	 0.145764 	 380.769836 	 0.297347 	 0.156161 	 0.072291 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.111796 	 280.057220 	 0.242060 	 0.109743 	 0.051158 
    r(raw):			 0.198793 	 136.914734 	 0.368172 	 0.208052 	 0.118880 
    averaged(raw):		 0.155294 	 208.485977 	 0.305116 	 0.158898 	 0.085019 
    
    l(filter):		 0.223213 	 98.047295 	 0.402375 	 0.237320 	 0.137154 
    r(filter):		 0.279176 	 111.412590 	 0.485342 	 0.311199 	 0.179664 
    averaged(filter):	 0.251194 	 104.729942 	 0.443858 	 0.274260 	 0.158409 
    ```
  
  - Epoch 30 hd#83,84, testmodel v8
  
    ```log
    no type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.057969 	 840.809998 	 0.129092 	 0.052477 	 0.020571 
    r(raw):			 0.161696 	 289.226044 	 0.329913 	 0.184892 	 0.078911 
    averaged(raw):		 0.109833 	 565.018005 	 0.229503 	 0.118685 	 0.049741 
    
    l(filter):		 0.094883 	 483.515686 	 0.205560 	 0.093179 	 0.040799 
    r(filter):		 0.221460 	 264.297821 	 0.408043 	 0.255644 	 0.128604 
    averaged(filter):	 0.158171 	 373.906738 	 0.306802 	 0.174411 	 0.084701 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.115450 	 277.878906 	 0.250024 	 0.116486 	 0.052770 
    r(raw):			 0.212554 	 138.626312 	 0.378628 	 0.232874 	 0.126893 
    averaged(raw):		 0.164002 	 208.252609 	 0.314326 	 0.174680 	 0.089832 
    
    l(filter):		 0.230306 	 99.242599 	 0.406821 	 0.244845 	 0.146145 
    r(filter):		 0.296843 	 113.699013 	 0.488908 	 0.332356 	 0.198866 
    averaged(filter):	 0.263574 	 106.470810 	 0.447865 	 0.288601 	 0.172506 
    ```
  
- 2019-11-05
  
  - The prediction is almost a constant by each relation. Guess it's because there are to few training examples. Will try to reduce the complicity of model. (Reduce the dimension)
    - <img src="https://i.imgur.com/GAfRQhN.png" style="zoom:80%;" />
  
- 2019-11-04
  
  - Export csv killed, guess it's because the pandas dataframe consumes too much memory. Try to write csv directly row by row. 
  
- 2019-11-03
  
  - Try to test on epoch 235 version 42 with the detail log print out.
    - Will use 
  ```python
  def predict_head_entity(self, t, r, k)
  ```
  ```python
  def predict_tail_entity(self, h, r, k)
  ```
  
- 2019-11-01
  
|              | MRR      | MR         | hit@10   |
| ------------ | -------- | ---------- | -------- |
| WV(l filter) | 0.071011 | 620.638794 | 0.151178 |
| WV(r filter) | 0.140395 | 691.752686 | 0.311395 |

- The above result doesn't seem so good. probably I should try this method with SGD.

  - Test version 10, hd#388 on epoch 84 of a training version 70(hd#386, hd#387) copy from version 69, but without entropy_tail. (Quick test)
  ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.042325 935.990417 0.098993 0.036109 0.011580
    r(raw): 0.002704 6182.238281 0.005668 0.001026 0.000195
    averaged(raw): 0.022514 3559.114258 0.052331 0.018567 0.005888

    l(filter): 0.071011 620.638794 0.151178 0.070849 0.027314
    r(filter): 0.005332 6161.775879 0.012753 0.004886 0.001075
    averaged(filter): 0.038171 3391.207275 0.081965 0.037868 0.014194
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.085583 752.860657 0.184110 0.083504 0.036255
    r(raw): 0.157229 709.406311 0.301622 0.168768 0.084188
    averaged(raw): 0.121406 731.133484 0.242866 0.126136 0.060222

    l(filter): 0.140616 479.035278 0.265318 0.148686 0.076322
    r(filter): 0.198594 689.133911 0.367194 0.224030 0.113114
    averaged(filter): 0.169605 584.084595 0.316256 0.186358 0.094718
  ```
- 2019-10-31
  - Test version 9, hd#383 on epoch 66 of a training(hd#381, hd#382) copy from version 69, but without entropy_head. (Quick test)
  ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.002571 5255.177734 0.004007 0.000391 0.000000
    r(raw): 0.104881 720.844482 0.245236 0.102560 0.039138
    averaged(raw): 0.053726 2988.011230 0.124621 0.051476 0.019569

    l(filter): 0.004469 5027.974609 0.010945 0.002345 0.000391
    r(filter): 0.140395 691.752686 0.311395 0.148588 0.061321
    averaged(filter): 0.072432 2859.863770 0.161170 0.075467 0.030856
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.067679 1650.723389 0.165396 0.062201 0.021890
    r(raw): 0.209750 527.325439 0.335972 0.221538 0.141259
    averaged(raw): 0.138715 1089.024414 0.250684 0.141869 0.081574

    l(filter): 0.107705 1459.979248 0.220365 0.113896 0.051305
    r(filter): 0.282739 498.307983 0.441464 0.307925 0.200967
    averaged(filter): 0.195222 979.143616 0.330915 0.210911 0.126136
  ```
  - Summary

|               | MRR      | MR         | hit@10   |
| ------------- | -------- | ---------- | -------- |
| TransE 100d   | 0.159739 | 352.164246 | 0.316256 |
| TransE 200d   | 0.146097 | 416.756073 | 0.307754 |
| DistMult 100d | 0.156411 | 690.129272 | 0.290946 |
| DistMult 200d | 0.151856 | 959.580688 | 0.277802 |
  - Test DistMult with 200 dimension  hd#43,44
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.056453 1149.785156 0.126307 0.049350 0.021206
      r(raw): 0.079204 1188.589111 0.171797 0.074905 0.033226
      averaged(raw): 0.067829 1169.187134 0.149052 0.062127 0.027216

      l(filter): 0.137365 760.768005 0.257598 0.145558 0.078130
      r(filter): 0.166348 1158.393433 0.298006 0.179908 0.102951
      averaged(filter): 0.151856 959.580688 0.277802 0.162733 0.090540
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.109459 721.569092 0.248070 0.109059 0.047200
      r(raw): 0.174974 274.194458 0.325613 0.177221 0.101485
      averaged(raw): 0.142216 497.881775 0.286842 0.143140 0.074343

      l(filter): 0.213861 388.225983 0.383807 0.232337 0.130705
      r(filter): 0.272162 244.112289 0.465455 0.296296 0.179468
      averaged(filter): 0.243012 316.169128 0.424631 0.264316 0.155086
    ```
  - Test DistMult with 100 dimension  hd#41,42
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.062519 962.039856 0.138132 0.056533 0.023502
      r(raw): 0.095997 826.936584 0.199990 0.092495 0.042949
      averaged(raw): 0.079258 894.488220 0.169061 0.074514 0.033226

      l(filter): 0.134531 581.389404 0.263412 0.139695 0.072511
      r(filter): 0.178291 798.869080 0.318479 0.194420 0.108717
      averaged(filter): 0.156411 690.129272 0.290946 0.167058 0.090614
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.115580 660.358398 0.249878 0.116535 0.053015
      r(raw): 0.188396 242.096115 0.343887 0.196130 0.110183
      averaged(raw): 0.151988 451.227264 0.296883 0.156332 0.081599

      l(filter): 0.209478 331.523712 0.380387 0.230822 0.126160
      r(filter): 0.280953 214.115845 0.469608 0.311981 0.185576
      averaged(filter): 0.245215 272.819763 0.424998 0.271401 0.155868  
    ```
  - Test TransE with 100 dimension  hd#39,40
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.055977 803.817139 0.139206 0.052575 0.012215
      r(raw): 0.105901 286.231659 0.280465 0.119613 0.019349
      averaged(raw): 0.080939 545.024414 0.209836 0.086094 0.015782

      l(filter): 0.105356 444.734039 0.241913 0.116290 0.035718
      r(filter): 0.214123 259.594452 0.390599 0.233509 0.127724
      averaged(filter): 0.159739 352.164246 0.316256 0.174900 0.081721
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.113079 621.246765 0.244063 0.114629 0.052184
      r(raw): 0.201579 211.163589 0.361331 0.214502 0.121616
      averaged(raw): 0.157329 416.205170 0.302697 0.164566 0.086900

      l(filter): 0.195237 309.619598 0.363188 0.210398 0.115020
      r(filter): 0.280849 185.357422 0.469364 0.310808 0.185087
      averaged(filter): 0.238043 247.488510 0.416276 0.260603 0.150054
    ```
- 2019-10-30
  - Test TransE with 200 dimension  hd#37,38
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.048065 922.038879 0.130363 0.043682 0.006108
      r(raw): 0.093585 311.879944 0.274113 0.105052 0.005130
      averaged(raw): 0.070825 616.959412 0.202238 0.074367 0.005619

      l(filter): 0.084330 549.759094 0.224372 0.097918 0.011287
      r(filter): 0.207864 283.753052 0.391137 0.225007 0.122154
      averaged(filter): 0.146097 416.756073 0.307754 0.161463 0.066720
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.105139 711.484131 0.228965 0.104466 0.047298
      r(raw): 0.191627 232.216110 0.344083 0.200381 0.115411
      averaged(raw): 0.148383 471.850128 0.286524 0.152424 0.081354

      l(filter): 0.181273 390.134186 0.340663 0.195837 0.103196
      r(filter): 0.269612 205.036209 0.455683 0.295172 0.177123
      averaged(filter): 0.225442 297.585205 0.398173 0.245505 0.140159
    ```
- 2019-10-29
  - Test version 8 duplated from version 5, on epoch 666 of version 68, hd#342 (wv + good type constrain)
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.029710 1263.511841 0.072413 0.018519 0.004788
      r(raw): 0.079407 488.204010 0.193883 0.070019 0.022965
      averaged(raw): 0.054559 875.857910 0.133148 0.044269 0.013877

      l(filter): 0.049523 869.156921 0.117268 0.041679 0.012264
      r(filter): 0.111769 455.589325 0.275530 0.109352 0.039578
      averaged(filter): 0.080646 662.373108 0.196399 0.075515 0.025921
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.096259 900.354858 0.222760 0.095866 0.039627
      r(raw): 0.210594 333.396118 0.364996 0.212597 0.140428
      averaged(raw): 0.153427 616.875488 0.293878 0.154231 0.090027

      l(filter): 0.194554 564.312012 0.341933 0.213036 0.120102
      r(filter): 0.333004 300.893127 0.512020 0.369344 0.241865
      averaged(filter): 0.263779 432.602570 0.426976 0.291190 0.180983
    ```
- 2019-10-29

  - Test version 7 duplated from version 5, on epoch 666 of version 68, hd#338

    
    - hd#33,34 DISTMULT + good type constrain
      ```log
      no type constraint results:
      metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
      l(raw):			 0.042927 	 1649.396606 	 0.098505 	 0.035376 	 0.014023 
      r(raw):			 0.055903 	 2170.710693 	 0.123962 	 0.049497 	 0.021841 
      averaged(raw):		 0.049415 	 1910.053711 	 0.111233 	 0.042436 	 0.017932 
      ```

    l(filter):		 0.133852 	 1254.703369 	 0.240399 	 0.144190 	 0.080817 
    r(filter):		 0.145487 	 2137.959717 	 0.249487 	 0.157774 	 0.094254 
    averaged(filter):	 0.139669 	 1696.331543 	 0.244943 	 0.150982 	 0.087535 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
    l(raw):			 0.095674 	 859.864929 	 0.227841 	 0.093130 	 0.036451 
    r(raw):			 0.157486 	 359.418365 	 0.289016 	 0.155624 	 0.091518 
    averaged(raw):		 0.126580 	 609.641663 	 0.258429 	 0.124377 	 0.063984 
    
    l(filter):		 0.213923 	 523.850952 	 0.379849 	 0.233412 	 0.133490 
    r(filter):		 0.261142 	 326.813049 	 0.436529 	 0.281638 	 0.174338 
    averaged(filter):	 0.237532 	 425.332001 	 0.408189 	 0.257525 	 0.153914 
    
    ```
    
    ```
  - Modify n-n.py in FB15K237 to use only train file for the type constrain
  - hd#31,32. DISTMULT + type constrain on FB15K237
    ```log
    no type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
  l(raw):			 0.042553 	 1672.544922 	 0.099629 	 0.033812 	 0.014072 
    r(raw):			 0.056930 	 2201.929443 	 0.123326 	 0.050718 	 0.023014 
    averaged(raw):		 0.049741 	 1937.237183 	 0.111478 	 0.042265 	 0.018543 
  
    l(filter):		 0.134185 	 1278.080566 	 0.242500 	 0.145119 	 0.080915 
    r(filter):		 0.146643 	 2169.195068 	 0.252077 	 0.160021 	 0.094058 
    averaged(filter):	 0.140414 	 1723.637817 	 0.247288 	 0.152570 	 0.087487 
    type constraint results:
    metric:			 MRR 		 MR 		 hit@10 	 hit@3  	 hit@1 
  l(raw):			 0.098334 	 289.780426 	 0.234975 	 0.096550 	 0.036206 
    r(raw):			 0.157488 	 137.375793 	 0.293707 	 0.156015 	 0.089417 
    averaged(raw):		 0.127911 	 213.578110 	 0.264341 	 0.126283 	 0.062811 
  
    l(filter):		 0.263767 	 92.576958 	 0.444005 	 0.284227 	 0.175559 
    r(filter):		 0.265738 	 104.675659 	 0.448891 	 0.288283 	 0.174387 
    averaged(filter):	 0.264752 	 98.626312 	 0.446448 	 0.286255 	 0.174973 
    ```
  - hd#29,30. DISTMULT + type constrain on FB15K
  ```log
    no type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.214292 667.139832 0.431498 0.238730 0.117316
    r(raw): 0.232288 792.698792 0.455469 0.258993 0.132332
    averaged(raw): 0.223290 729.919312 0.443483 0.248862 0.124824
  
    l(filter): 0.548831 473.137329 0.705236 0.616715 0.455300
    r(filter): 0.571994 671.308472 0.712041 0.626907 0.493102
    averaged(filter): 0.560413 572.222900 0.708639 0.621811 0.474201
    type constraint results:
    metric: MRR MR hit@10 hit@3 hit@1
    l(raw): 0.338014 119.553185 0.592220 0.392934 0.215944
    r(raw): 0.291746 155.543335 0.551455 0.325253 0.174468
    averaged(raw): 0.314880 137.548264 0.571837 0.359093 0.195206
  
    l(filter): 0.718599 22.662897 0.842393 0.762388 0.648592
    r(filter): 0.664309 34.289669 0.827225 0.727108 0.572379
    averaged(filter): 0.691454 28.476284 0.834809 0.744748 0.610486
  ```


- 2019-10-21
  - Test DistMult on FB15K, openke version4, hd#6,7
    ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.216304 671.843933 0.430804 0.241777 0.119653
      r(raw): 0.232948 801.569702 0.454876 0.260026 0.133636
      averaged(raw): 0.224626 736.706787 0.442840 0.250901 0.126644

      l(filter): 0.546768 477.942474 0.704305 0.615869 0.453167
      r(filter): 0.571171 680.167419 0.710315 0.626500 0.492238
      averaged(filter): 0.558970 579.054932 0.707310 0.621185 0.472702
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.339739 120.093071 0.592473 0.393950 0.218652
      r(raw): 0.390622 77.666466 0.662135 0.453573 0.258824
      averaged(raw): 0.365181 98.879768 0.627304 0.423761 0.238738

      l(filter): 0.716952 23.255760 0.841817 0.760983 0.646510
      r(filter): 0.739577 17.122192 0.867905 0.787087 0.666503
      averaged(filter): 0.728264 20.188976 0.854861 0.774035 0.656507
    ```
  
- 2019-10-17
  - Train and test TransE and DistMult with Type Constraint
    - Result
  
      | Model    | MRR      | MR        | hit@10   |
      | -------- | -------- | --------- | -------- |
      | TransE   | 0.276643 | 94.304749 | 0.463378 |
      | DistMult | 0.302933 | 73.647980 | 0.499145 |
    - hd#2,3 TransE
      ```log
        metric: MRR MR hit@10 hit@3 hit@1
        l(raw): 0.042457 1232.688965 0.117072 0.036206 0.004446
        r(raw): 0.086375 424.292572 0.256718 0.095866 0.004446
        averaged(raw): 0.064416 828.490784 0.186895 0.066036 0.004446

        l(filter): 0.074815 852.634521 0.211522 0.090736 0.004446
        r(filter): 0.186090 394.911987 0.370908 0.205023 0.097821
        averaged(filter): 0.130453 623.773254 0.291215 0.147879 0.051134
        type constraint results:
        metric: MRR MR hit@10 hit@3 hit@1
        l(raw): 0.097070 312.879547 0.218851 0.092104 0.040506
        r(raw): 0.248925 79.947281 0.437946 0.260432 0.159826
        averaged(raw): 0.172998 196.413422 0.328398 0.176268 0.100166

        l(filter): 0.215429 122.863380 0.381120 0.228086 0.135298
        r(filter): 0.337856 65.746117 0.545637 0.369198 0.237125
        averaged(filter): 0.276643 94.304749 0.463378 0.298642 0.186211
      ```
    - hd#4,5 DistMult
      ```log
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.042414 1696.429810 0.098309 0.033812 0.013877
      r(raw): 0.057320 2223.440186 0.126063 0.051207 0.023356
      averaged(raw): 0.049867 1959.935059 0.112186 0.042510 0.018616

      l(filter): 0.132650 1301.728882 0.240692 0.142969 0.080231
      r(filter): 0.146626 2190.695312 0.250122 0.159533 0.095133
      averaged(filter): 0.139638 1746.212158 0.245407 0.151251 0.087682
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1
      l(raw): 0.097769 291.067322 0.232825 0.095085 0.036255
      r(raw): 0.218119 69.893234 0.409069 0.228134 0.126942
      averaged(raw): 0.157944 180.480286 0.320947 0.161610 0.081599

      l(filter): 0.262460 93.742744 0.444444 0.281980 0.173996
      r(filter): 0.343407 53.553211 0.553845 0.376380 0.238347
      averaged(filter): 0.302933 73.647980 0.499145 0.329180 0.206171
      ```



- 2019-10-12

  - test version 5 on epoch 666 of version 68, hd#335
    - result
      - no type constraint results:
      - metric: MRR MR hit@10 hit@3 hit@1
      - l(raw): 0.029710 1263.511841 0.072413 0.018519 0.004788
      - r(raw): 0.079407 488.204010 0.193883 0.070019 0.022965
      - averaged(raw): 0.054559 875.857910 0.133148 0.044269 0.013877
      - 
      - l(filter): 0.049523 869.156921 0.117268 0.041679 0.012264
      - r(filter): 0.111769 455.589325 0.275530 0.109352 0.039578
      - averaged(filter): 0.080646 662.373108 0.196399 0.075515 0.025921
      - type constraint results:
      - metric: MRR MR hit@10 hit@3 hit@1
      - l(raw): 0.098260 305.326019 0.225838 0.096941 0.040066
      - r(raw): 0.272824 76.951233 0.467409 0.292876 0.181276
      - averaged(raw): 0.185542 191.138626 0.346624 0.194909 0.110671
      - 
      - l(filter): 0.242131 108.164177 0.412880 0.262533 0.157774
      - r(filter): 0.416580 60.661144 0.599531 0.455145 0.323512
      - averaged(filter): 0.329356 84.412659 0.506205 0.358839 0.240643

- 2019-10-11

  - Version 68 folks version 67, change alpha from 0.8 to 0.9, (hd#327,#328), Ends at Epoch: 666, loss: 4698.188301563263,
  - version 69 folks version 66, train on adam 0.001 (hd#329,#330),Ends at Epoch: 151, loss: 9055.616076946259,
  - test version 2, test on epoch 263 of version 66, hd#332,
    - no type constraint results:
    - metric: MRR MR hit@10 hit@3 hit@1
    - l(raw): 0.021713 1208.497437 0.053015 0.005521 0.000489
    - r(raw): 0.051533 508.821014 0.148832 0.023454 0.003420
    - averaged(raw): 0.036623 858.659241 0.100923 0.014487 0.001954
    - 
    - l(filter): 0.037891 816.116211 0.101485 0.020375 0.004056
    - r(filter): 0.071125 476.089844 0.230431 0.041874 0.007622
    - averaged(filter): 0.054508 646.103027 0.165958 0.031125 0.005839
    - type constraint results:
    - metric: MRR MR hit@10 hit@3 hit@1
    - l(raw): 0.102386 289.001709 0.233949 0.101290 0.041044
    - r(raw): 0.281420 69.016174 0.480749 0.306557 0.185478
    - averaged(raw): 0.191903 179.008942 0.357349 0.203924 0.113261
    - 
    - l(filter): 0.270987 92.826538 0.449624 0.293804 0.182938
    - r(filter): 0.458211 52.676880 0.638864 0.503616 0.363579
    - averaged(filter): 0.364599 72.751709 0.544244 0.398710 0.273258

- 2019-10-10

  - Version 63 folks version 62, alpha change from 0.6 to 0.7, (hd#319,320), final loss at epoch 661, loss 4773
  - Version 65 folks version 61, train on adam 0.005 (hd#321,#322), 3 hrs 37 mihs, final loss at epoch 136, loss 9751.
  - Version 66 folks version 65, train on adam 0.002 (hd#323,#324), end at epoch 263, loss 9116
  - Version 67 folks version 63, alpha change from 0.7 to 0.8, (hd#325,#326), end at epoch 668, loss 4736

- 2019-10-09

  - version 59, upload the code of version 47, alpha change from 0.5 to 0.4, hd#311-312 is worer than Version 47
  - version 60, upload the code of version 47, alpha change from 0.5 to 0.1, hd#313-314 is worse than version 59
  - Dimension 200, adam alpha 0.01 final loss 11687 is better than alpha 0.02 final loss 14335 (hd#310,hd#307)
  - Version 61, train on adam 0.008 (hd#315,#316), 3hrs 38mins, final loss at epoch 136, 10775.
  - Version 62 folks version 47, alpha change from 0.5 to 0.6, (hd#317,hd#318), at final loss 4835

- 2019-10-08

  - Version 57, test on version 54 epoche 103 with loss 4857

- 2019-10-07

  - Version 42 training result seems very good. It ends at loss 65.
  - Version 43 test on version 42 epoch 235.
  - Version 47 the same as version 42, training without hinge loss nor negative sampling, 9 hours stopped at epoche 664. hd#281
  - Version 54 continue training of version 47 from epoche 664.

- 2019-10-06

  - Change beta to 0.001 from 0.01 , loss becomes nan at epoch 3
  - Change alpha to 0.1, loss decreases slowly and bounces a lot.
  - Change alpha to 0.05, loss decreases slowly and bounces a lot
  - Change alpha to 0.02, loss decreases slowly and bounces a lot
  - Change alpha to 0.01, loss decreases slowly and bounces a lot
  - It seems alpha 0.008 works best.
  - Remove the negative sampling, loss 9k, doesn't decrease.
  - Try beta 0.0001, loss 7k, doesn't decrease
  - Try hinge loss without the negative sampling, loss 10k doesn't decrease
  - Alpha 0.2, Try hinge loss without the negatives ampling, loss 10k doesn't decrease
  - Vesion 42, return back to the begining. SGD, with negative sampling. #266 on hyperdash

- 2019-10-05

  - Version 30 learning rate is too small
  - Version 34 training final loss 4k
    - Version 35 train on version 34 epoch 5

- 2019-10-04

  - In embedding.vec.json, the embeddings are all NaN
  - It's may be due to the exploding gradient problem
    - https://stackoverflow.com/questions/51038266/why-my-tensorflow-model-outputs-become-nan-after-x-epochs

- 2019-10-02

  - test result openke_word2vec version 18
    - 1 hr 17mins
      no type constraint results: 
      metric: MRR MR hit@10 hit@3 hit@1 
      l(raw): 0.028714 1154.996826 0.077299 0.021597 0.003713 
      r(raw): 0.098217 554.068909 0.242793 0.113359 0.027802 
      averaged(raw): 0.063465 854.532837 0.160046 0.067478 0.015758 
      l(filter): 0.035007 849.547546 0.093472 0.028437 0.004007 
      r(filter): 0.108639 532.859009 0.268299 0.129972 0.030441 
      averaged(filter): 0.071823 691.203247 0.180885 0.079205 0.017224 
      type constraint results: 
      metric: MRR MR hit@10 hit@3 hit@1 
      l(raw): 0.091450 313.823456 0.186700 0.090003 0.041972 
      r(raw): 0.268340 105.762825 0.437555 0.285987 0.186602 
      averaged(raw): 0.179895 209.793137 0.312127 0.187995 0.114287 
      l(filter): 0.172822 161.105255 0.301524 0.176732 0.108619 
      r(filter): 0.321689 95.164764 0.513583 0.353806 0.230431 
      averaged(filter): 0.247255 128.135010 0.407554 0.265269 0.16952
  - `self.saver = tf.train.Saver(max_to_keep = 20)` to keep more checkpoints

- 2019-09-30

  - kaggle will rename test_openke to test-openke

- 2019-09-27

  - Test from checkpoint get loss array [nan nan nan ... nan nan nan]

- 2019-09-25

  - Different opt method will affect the loss of first epoch?
    - SGD: ~900
    - adam: ~65
  - Without negative sampling and margin loss, the learning rate is quite hard to tune.

- 2019-09-24

  - performance result
    - 3 hrs 55mins
      no type constraint results: metric: MRR MR hit@10 hit@3 hit@1 l(raw): 0.035857 3088.653564 0.073634 0.036646 0.016417 r(raw): 0.176729 1972.872314 0.285498 0.186798 0.127919 averaged(raw): 0.106293 2530.762939 0.179566 0.111722 0.072168 l(filter): 0.045567 2856.652100 0.082967 0.048422 0.024382 r(filter): 0.215069 1953.115356 0.339734 0.237565 0.158311 averaged(filter): 0.130318 2404.883789 0.211351 0.142993 0.091347 type constraint results: metric: MRR MR hit@10 hit@3 hit@1 l(raw): 0.071169 640.897400 0.138132 0.068748 0.034154 r(raw): 0.230529 233.559174 0.362846 0.246116 0.163491 averaged(raw): 0.150849 437.228271 0.250489 0.157432 0.098822 l(filter): 0.158843 408.915131 0.252077 0.163100 0.109010 r(filter): 0.301488 213.801620 0.449477 0.327421 0.226717 averaged(filter): 0.230165 311.358368 0.350777 0.245260 0.167864

- 2019-09-23

  - Download the model in kaggle

    - method 1
      `
      from IPython.display import FileLink, FileLinks
      FileLinks('.') #lists all downloadable files on server
      `

  - Performance result

    - 1 hr 33 mins
      no type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1 
      l(raw): 0.008282 4673.078125 0.009333 0.005375 0.005375 
      r(raw): 0.058605 3029.861084 0.119124 0.058292 0.022183 
      averaged(raw): 0.033444 3851.469727 0.064228 0.031833 0.013779 

      l(filter): 0.008357 4450.305664 0.009528 0.005375 0.005375 
      r(filter): 0.059085 3011.388672 0.119857 0.058292 0.022183 
      averaged(filter): 0.033721 3730.847168 0.064693 0.031833 0.013779 
      type constraint results:
      metric: MRR MR hit@10 hit@3 hit@1 
      l(raw): 0.064264 663.311951 0.130167 0.061859 0.028877 
      r(raw): 0.223904 241.954559 0.343741 0.230724 0.164370 
      averaged(raw): 0.144084 452.633240 0.236954 0.146291 0.096624 

      l(filter): 0.133146 440.550415 0.220805 0.136324 0.087462 
      r(filter): 0.271826 223.478745 0.418304 0.286377 0.202922 
      averaged(filter): 0.202486 332.014587 0.319554 0.211351 0.145192 

- 2019-09-22

  - https://github.com/googlecolab/colabtools/issues/661
    - Tesla T4 is not accessible any more for me on colab. According to my test, K80(colab) is about 6 times slower than P100(kaggle).
    - for openke_word2vec each epoch
      - colab 517s
      - kaggle 97s
      - vast.ai 188s (GTX 1070ti, P8Z68, Core i5-2500 2.0/4 cores 8/16 GB, Samsung 517MB/s, 6.2 DLPerf, $0.068/hr)
      - colab-cpu 7250s (around 2 hours)
  - Parameters for openke to run on colab and kaggle
    - colab
      - dimension = 100
      - nbatches = 1500
    - kaggle
      - dimension = 100
      - nbatches = 1000
  - openke_word2vec run on colab takes around 2 hours/epoch
    - Epoch: 0, loss: 200.00305849313736, time: 7250.3890788555145
    - Epoch: 1, loss: 199.99601310491562, time: 7256.021113157272
    - Epoch: 2, loss: 199.98996078968048, time: 7288.72056388855 Epoch: 3, loss: 199.98503702878952, time: 7285.004914283752
  - 

- 2019-09-19

  - Noise-contrastive estimation
    - https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss
      The issue
      There are some issues with learning the word vectors using an "standard" neural network. In this way, the word vectors are learned while the network learns to predict the next word given a window of words (the input of the network).
      Predicting the next word is like predicting the class. That is, such a network is just a "standard" multinomial (multi-class) classifier. And this network must have as many output neurons as classes there are. When classes are actual words, the number of neurons is, well, huge.
      A "standard" neural network is usually trained with a cross-entropy cost function which requires the values of the output neurons to represent probabilities - which means that the output "scores" computed by the network for each class have to be normalized, converted into actual probabilities for each class. This normalization step is achieved by means of the softmax function. Softmax is very costly when applied to a huge output layer.
      The (a) solution
      In order to deal with this issue, that is, the expensive computation of the softmax, Word2Vec uses a technique called noise-contrastive estimation. This technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E] to learn word embeddings from unlabelled natural language text.
      The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead.
      For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.
      This is important: instead of predicting the next word (the "standard" training technique), the optimized classifier simply predicts whether a pair of words is good or bad.
      Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often.
  - git largfe files
    - https://git-lfs.github.com/
  - OpenKE can't show output from the Base.so, which is the c library, in jupyter.
    - https://github.com/QuantStack/xeus-cling/issues/112
    - https://github.com/ipython/ipykernel/issues/110

- 2019-09-17

  - Get output embedding matrix in gensim `model.syn1`
    - https://stackoverflow.com/questions/41162876/get-weight-matrices-from-gensim-word2vec

- 2019-09-13

  - plotly bsample line
    - `trace2 = PlotlyJS.scatter(;x=x2,y=y2, mode="line", name="average mr",line= attr(shape "spline"),)`

- 2019-09-10

  - Predict a word using word2vec model
    - https://datascience.stackexchange.com/questions/9785/predicting-a-word-using-word2vec-model

- 2019-09-09

  - Make dataframe not truncated in jupyter
    - https://github.com/JuliaData/DataFrames.jl/issues/886
      When Julia displays a large data structure such as a matrix, by default it truncates the display to a given number of lines and columns. In IJulia, this truncation is to 30 lines and 80 columns by default. You can change this default by the LINES and COLUMNS environment variables, respectively, which can also be changed within IJulia via ENV (e.g. ENV["LINES"] = 60). (Like in the REPL, you can also display non-truncated data structures via print(x).)
    - `ENV["COLUMNS"] = 160`

- 2019-09-05

  - entity 14516 exist in test but not in train for FB15K-237.

- 2019-09-03

  - xp152 DistMult FB15K
  - xp153 TranseE FB15K
  - Redo xp152,xp153 at xp161,xp162 but use all the test examples instead of 10000

- 2019-08-27

  - [Paper with code WN18RR](https://paperswithcode.com/sota/link-prediction-on-wn18rr)

  - [machine epsilon in c language](https://bytes.com/topic/c/answers/670460-machine-epsilon)

  - REAL is defined as float in ./OpenKE/base/Setting.h

  - Modify Test.h in testHead() and testTail()
    ` for (INT j = 0; j < entityTotal; j++) {
    REAL value = con[j];
    if (j != h && value < minimal) {
    l_s += 1;
    if (not _find(j, t, r))
    l_filter_s += 1;
    }
    }`
    ==>
    `
    \#include "float.h"
    ....

    for (INT j = 0; j < entityTotal; j++) {
    REAL value = con[j];
    if (j != h && value < minimal + FLT_EPSILON) {
    l_s += 1;
    if (not _find(j, t, r))
    l_filter_s += 1;
    }
    }`

  - Test xp152 the best DISTMULT over FB15K with epsilon as xp160

    - The strick rank doesn't affect DISTMULT

- 2019-08-14

  - Parle avec Philippe
    - Try strict ranking in TransE,DISTMULT
    - Try LinkFeat on FB15K-selected
    - Optional try the Naive Bayse with link and node feat.

- 2019-08-12

  - There are some rank #1 with a deviation very small
    (14946, 0.030303573)
    (1, 5.5513004e-17) 
    (1, 5.5513004e-17) 
    (2, 0.0014666952) 
    (1, 0.023063548) 
    (14929, 0.040659655)
    (1, 0.0013072524) 
    (18, 0.026367811) 
    (18, 0.026367811) 
    (1, 0.014130712) 
    (9, 0.016517866) 

- 2019-08-11

  - Check the test method in OpenKE.

    - https://github.com/thunlp/OpenKE/blob/master/base/Test.h
      `
      REAL minimal = con[h];
      INT l_s = 0;
      INT l_filter_s = 0;
      INT l_s_constrain = 0;
      INT l_filter_s_constrain = 0;

      for (INT j = 0; j < entityTotal; j++) {
      if (j != h) {
      REAL value = con[j];
      if (value < minimal) {
      l_s += 1;
      if (not _find(j, t, r))
      l_filter_s += 1;
      }
      while (lef < rig && head_type[lef] < j) lef ++;
      if (lef < rig && j == head_type[lef]) {
      if (value < minimal) {
      l_s_constrain += 1;
      if (not _find(j, t, r)) {
      l_filter_s_constrain += 1;
      }
      } 
      }
      }
      }

      
      `

    - If every triple gets the same score, than they all will be ranked as #1

- 2019-08-09

  - GC.gc() doesn't work in the same function
    `function foo()
    A = zeros(Int64,10^9)
    A = nothing
    GC.gc() # Doesn't work
    end
    foo()`

- 2019-08-08

  - In `_lookup_feature()`, annotate `feat_idx` and `feat_idx_inv` as Float32

  - dictionary of tuple , string , int `haskey()` performance.

    `A = collect(1:70000)

    dict1 = Dict{Tuple{Int64,Int64},Int64}()

    dict2 = Dict{String,Int64}()

    dict3 = Dict{Int64,Int64}()

    n = length(A)

    shift = ceil(log10(n))

    for i in 1:length(A)

    dict1[(i,i)]=i

    dict2["$i&$i"]=i

    dict3[i*10^shift+i]=i

    end

    x = 100

    @btime haskey(dict1,(x,x))

    @btime haskey(dict2,"$x&$x")

    @btime haskey(dict3,x*10^shift+x)

    `

    223.346 ns (1 allocation: 32 bytes)

    1.192 μs (9 allocations: 400 bytes)

    103.101 ns (3 allocations: 48 bytes)

    - if we apply this we will save 100ns for each test mask(about 15000 entites) over 50000 test triples. So it will save totally 0.0000001*15000*50000 = 750s. It doesn't worth it

  - We may use `GC.enable(false)` to disable the gc collector and enable it after with `GC.enable(true)`. And run the collector aLLLLLLny time with `GC.gc()`
    `GC.enable(false)
    test_once([0,348,0])
    GC.enable(true)
    GC.gc()`

- 2019-08-07

  - optimise `get_test_candidats()`
    method 1: 811.356 μs (14968 allocations: 2.74 MiB)
    `if TEST_ARG == "TAIL"
    test_once_candidats = hcat([ [arg1,mask,rel] for mask in entity_list]...)
    else
    test_once_candidats = hcat([ [mask,arg2,rel] for mask in entity_list]...)
    end`

    method 2: 14.572 ms (221716 allocations: 5.78 MiB)
    `if TEST_ARG == "TAIL"
    for i in 1:length(entity_list)
    test_once_candidats[:,i] = [arg1;entity_list[i];rel]
    end
    else
    for i in length(entity_list)
    test_once_candidats[:,i] = [entity_list[i];arg2;rel]
    end
    end`

    method 3: 78.292 μs (15 allocations: 935.08 KiB)
    `if TEST_ARG == "TAIL"
    test_once_candidats = vcat(fill(arg1,n),entity_list,fill(rel,n))
    else
    test_once_candidats = vcat(entity_list,fill(arg2,n),fill(rel,n))
    end`

- 2019-08-05

  - How to deal with the rank of entities with the same score?
  - Add l1 penalty, it shows `back!` was already used error during training.

- 2019-08-02

  - `isInTrain()` dataframe version takes 1.25ms while set version takes 72.584ns ONLY!
    ` return size(df_train[(df_train.arg1 .== arg1) .& 
    (df_train.rel .== rel) .& 
    (df_train.arg2 .== arg2),:],1) > 0`

    vs

    `return (arg1,arg2,rel) in train_triple_set`

  - broadcasting is quite slower than loop

    - https://github.com/JuliaLang/julia/issues/28126

  - Lookup value

    - dictionary vs array vs dataframe
      - @btime feat_tuple_dict[(134,172)]
        - 34.760 ns (1 allocation: 16 bytes)
      - @btime findall(x->x==(134,172), A)
        - 2.386 μs (6 allocations: 272 bytes)
      - @btime df_feat_idx[in(["134&172"]).(df_feat_idx.label),:]
        - 34.771 μs (34 allocations: 6.69 KiB)

- 2019-08-01

  - How to properly mix the operation between Tracked Array and Array of TrackeReal
    - https://github.com/FluxML/Flux.jl/issues/205
    - Tracker.collect()
  - Training is very slow. 100 examples take 6 mins. Then half million examples will take 500 hours (20 days)

- 2019-07-31

  - julia broadcasting problem
    `A = collect(1:12)
    A = reshape(A,(3,:))'

    add(x,y,z) = x+y+z
    myadd = x -> add(x...)
    add.(eachcol(A)...)
    myadd.(eachrow(A))

    add.([1, 4, 7, 10],[2, 5, 8, 11],[3, 6, 9, 12])
    myadd.([[1, 2, 3],[4, 5, 6],[7, 8, 9],[10, 11, 12]])
    `

- 2019-07-23

  - Rendez-vous avec Philippe
    - Concentrer sur:
      - bias dans Frequence
      - bias dans donnes
      - graph (probablement, random walk)

- 2019-07-19

  - Use `corssentopy(x)` to replace the `-sum(log.(x))`
  - `sum((loss(d...) for d in data))` will consume much memory
    - Because the `loss(d...)` return a tracked value and accumulate this value will take lots memory
    - Use `sum((Tracker.data(loss(d...)) for d in data))` instead
  - Wrong loss
    - `loss(x) = max.(0,crossentropy(m(x),ones(1,size(x,2))/size(x,2)))`

- 2019-07-18

  - The length methode is quite important to the definition of iterator.

- 2019-07-16

  - Resolved the errors in training.
  - `xs[2:end,1]=1` is much slower than `xs[:,1]=1` when xs is big
  - `Flux.params()` works with those predefined layers like `Dense`

- 2019-07-08

  - `df_by_ent_pair = by(df_train, [:arg1,:arg2], rels = :rel=> x -> collect(x), rels_count = :rel=>length)` is incorrect!
    - `df_by_ent_pair = by(df_train, [:arg1,:arg2], rels = :rel=> x -> collect.([x]), rels_count = :rel=>length)`

- 2019-07-05

  - Link feature summary
    Summary Stats:
    Length: 394804
    Missing Count: 0
    Mean: 1.223752
    Minimum: 1.000000
    1st Quartile: 1.000000
    Median: 1.000000
    3rd Quartile: 1.000000
    Maximum: 10.000000
    Type: Int64

- 2019-07-04

  - Construct Link and node feature
    - https://www.overleaf.com/read/gfkywhhpwtfn

- 2019-06-26

  - Finish relation frequence distribution hitorgram on FB15K-237
  - Found [qgrid](https://github.com/quantopian/qgrid)to interact with table grid

- 2019-06-25

  - plotlyjs.jl
    - SyncPlots example doesn't work
      - http://spencerlyon.com/PlotlyJS.jl/syncplots/#syncplots

- 2019-06-22

  - `cp -r WebIO.jl-master/packages/webio/ ./.julia/packages/WebIO/nNJPv/packages/`

  - in julia pkg mode, `build WebIO` no error but it shows still `WebIO not detected.`
    `using WebIO
    display(Node(:input, attributes = Dict("type" => "range")))` shows the rang input bar

  - Discussion related to this problem

    - https://github.com/sglyon/PlotlyJS.jl/issues/255
    - https://github.com/sglyon/PlotlyJS.jl/issues/278#issuecomment-499970113
    - https://github.com/JuliaGizmos/WebIO.jl/issues/281

  - plotly offline charts show blank

    - code
      `
      def enable_plotly_in_cell():
      import IPython
      from plotly.offline import init_notebook_mode
      display(IPython.core.display.HTML('''
      <script src="/static/components/requirejs/require.js"></script>
      '''))
      init_notebook_mode(connected=False)

      from plotly.offline import iplot
      import plotly.graph_objs as go

      enable_plotly_in_cell()

      data = [
      go.Contour(
      z=[[10, 10.625, 12.5, 15.625, 20],
      [5.625, 6.25, 8.125, 11.25, 15.625],
      [2.5, 3.125, 5., 8.125, 12.5],
      [0.625, 1.25, 3.125, 6.25, 10.625],
      [0, 0.625, 2.5, 5.625, 10]]
      )
      ]
      iplot(data)
      `

    - In browser console, it shows:

      - `Uncaught ReferenceError: require is not defined`

    - Jupyterlab and Plotly offline: requirejs is not defined

    - https://stackoverflow.com/questions/54148668/jupyterlab-and-plotly-offline-requirejs-is-not-defined

    - Follow the instruction to install the [plotly extension for Jupyter Lab](https://github.com/jupyterlab/jupyter-renderers/tree/master/packages/plotly-extension) it resolves the problems. (It resolves the WebIO not dected problem)

- 2019-06-21

  - Encounter WebIO not detected problem.

  - command log

    - `conda install -c conda-forge nodejs`

    - `npm install --save core-js@^3`

    - `jupyter labextension install @webio/jupyter-lab-provider`

      - Segmentation fault (core dumped)
      - error Command failed with exit code 139.

    - `jupyter lab build`

      - Segmentation fault (core dumped)
      - error Command failed with exit code 139.
      - Node v6.12.2 resolved the jupyter lab build problem

    - `npm i @webio/webio`

    - `pkg> build WebIO`

      Errored, use --debug for full output:

      ValueError: Can install "/u/wujieche/.julia/packages/WebIO/nNJPv/packages/webio" only link local directories

      [ Info: Copying WebIO nbextension files to /u/wujieche/.local/share/jupyter/nbextensions/webio.

      ┌ Warning: Unable to install Jupyter Lab extension; make sure jupyter is in your PATH and rebuild WebIO if you want to use Jupyter Lab.

      │ exception = failed process: Process(`/u/wujieche/miniconda3/bin/jupyter labextension link --no-build /u/wujieche/.julia/packages/WebIO/nNJPv/packages/webio`, ProcessExited(1)) [1]

      └ @ Main ~/.julia/packages/WebIO/nNJPv/deps/build.jl:9

      - I believe it's because folder `/u/wujieche/.julia/packages/WebIO/nNJPv/packages/webio` doesn't exist.
      - https://github.com/JuliaGizmos/WebIO.jl/tree/18d6c6bbcf2eaab19fab3de6ada4e3339ac3e6b5/packages/webio

- 2019-05-30

  - Initial

     

    Apache Jena Fuseki

    - The local machine path `/part/01/Tmp/Freebase/Index` may not work. Use `/data/rali6/Tmp/Freebase/Index` instead.
    - Resolve [ERROR Exception in initialization:GC overhead limit exceeded](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=I3lcYzwLLRUmFg6d23jKX8de)

  - Try SPARQL

    - actor <-> film

      - `12 years a slave freebase mid: m.0h32y7j / wikidata id:Q3023357`

      - `Brad Pitt freebase mid: m.0c6qh / wikidata id: Q35332`

      - Aprroved [The performance.film and performance.actor should be switched](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=YuEgfruWp8ENZrFPEjncGX2w) [Local jena link](http://localhost:8080/dataset.html?query= PREFIX+freebase%3A+ SELECT+* WHERE+{ ++%3Factor+freebase%3Atype.object.name+"Brad+Pitt"%40en. ++%3Factor+freebase%3Afilm.actor.film+%3Fmed. ++%3Fmed+freebase%3Afilm.performance.film+%3Ffilm. ++%3Ffilm+freebase%3Atype.object.name+"12+Years+a+Slave"%40en. } LIMIT+25 )

        PREFIX fb: <http://rdf.freebase.com/ns/>
        SELECT *
        WHERE {
        ?actor fb:type.object.name "Brad Pitt"@en.
        ?actor fb:film.actor.film ?med.
        ?med fb:film.performance.film ?film.
        ?film fb:type.object.name "12 Years a Slave"@en.
        }

        ///////////////////////////////////////////////////////////////////
        // obsolete

        PREFIX freebase: <http://rdf.freebase.com/ns/>
        SELECT *
        WHERE {
        freebase:m.0c6qh freebase:film.actor.film ?med.
        ?med freebase:film.performance.film freebase:m.0h32y7j
        }
        LIMIT 25

        
        SELECT *
        WHERE {
        <http://rdf.freebase.com/ns/m.0c6qh> <http://rdf.freebase.com/ns/film.actor.film> ?med.
        ?med <http://rdf.freebase.com/ns/film.performance.film> <http://rdf.freebase.com/ns/m.0h32y7j>
        }
        LIMIT 25
        // <http://rdf.freebase.com/ns/m.0hgdwjl>

        
        SELECT *
        WHERE {
        <http://rdf.freebase.com/ns/m.0h32y7j> <http://rdf.freebase.com/ns/type.object.name> ?o.
        FILTER (lang(?o) = 'en')
        }
        LIMIT 25
        // "12 Years a Slave"@en

    - award <-> nominee

  - Upload freebase to wikidata mapping to fuseki

    - 

- 2019-05-29

  - Found CVT relation graph [CVT relation](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=4z0QhWzO2PnDdm8UqbCYy994)

- 2019-05-28

  - Found freebase shema wiki page
    - [freebase-schema](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=bQOEAVbmTTYX4JPDgacQTxUr)

- 2019-05-27

  - Find example in Freebase easy
  - Data strange
  - Find archived freebase doc page
    - [Freebase Docs](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=alU1XVdeAqEB8sVDr_jCTUlP)
    - [Freebase Schema Explorer](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=pq02BoBUnR1ay3yqZBsHFzDW)

- 2019-05-24

  - Comme le Freebase est désapprouvé, peut-être wikidata est un meilleur choix? E.g.: WD40k and WD40k_nl from wikidata.

- 2019-05-23

  - succeeded loading wembedder's word2vec model

    - Should use get(model.wv,"Q2") instead of model.wv["Q2"\] Refer to this page [https://github.com/JuliaPy/PyCall.jl](https://github.com/JuliaPy/PyCall.jl)
      Given o::PyObject, get(o, key) is equivalent to o[key] in Python, with automatic type conversion. 

    - Read

       

      freebase guide page

      - /Domain_ID/Type_ID/Property_ID

    - data strange

      - Billy Boyd,/award/award_nominee/award_nominations./award/award_nomination/award_nominee,Sean Bean
      - 

- 2019-05-22

  - Downloaded the wembedder model(gensim word2vec) and try to load it in julia

- 2019-05-21

  - Wembedder web service

     

    Wembedder: Wikidata entity embedding web service

    - https://tools.wmflabs.org/wembedder/api/most-similar/Q2

- 2019-05-20

  - Found pretrained wikidata [Pretrained wikipedia embeddings with word2vec](https://dynalist.io/d/5xiJQiY9VxhxatwvZVDVPCyO#z=i3X6rUqXhciONPVXLYGwXr3d)

- 2019-05-17

  - Fabrizio found there are dot in the relatiosns
    - e.g: /award/award_nominee/award_nominations./award/award_nomination/award
    - It shows some relations contains two sub relations
  - /award/award_nominee & /award_nominee/award
  - Write indexation_kb.ipynb to seperate sub relations
    - Out in FB15k_sprel and FB15k-237_sprel
    - 1389 entities for FB15k_sprel
    - 302 entities for FB15K-237_sprel

- 2019-05-16

  - Finish lexicalize_fb15k.ipynb
  - Can't find the ralations with 97% intersections

- 2019-05-15

  - 50K valids
    - 13292 unique entities
  - There are some entities not included in Freebase to wikidata mappings
    - e.g: `/m/07kcvl`

- 2019-05-14

  - Found

     

    - [Freebase/Wikidata Mappings](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=D7yUNwGaYBiNyVsK5qWiZp9S)
    - and [wikidata API](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=G08zBucTUT0ovBLOsygBNuGE)

- 2019-05-13

  - FB Search API
    - [MID search API](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=XGhuyD_8DcYG_UyDYQzchl8S)
  - /m/06cx9 does not exist!
    - https://developers.google.com/apis-explorer/#p/kgsearch/v1/kgsearch.entities.search?ids=%252Fm%252F06cx9&_h=1&
      - return json is
        {
        "@context": {
        "@vocab": "http://schema.org/",
        "goog": "http://schema.googleapis.com/",
        "EntitySearchResult": "goog:EntitySearchResult",
        "detailedDescription": "goog:detailedDescription",
        "kg": "http://g.co/kg"
        },
        "@type": "ItemList",
        "itemListElement": []
        }
    - according to https://www.wikidata.org/wiki/Q7270it means republic

- 2019-05-12

  - Know more about FB15K source
    - [FB15K Source](https://dynalist.io/d/dNMnL9fQPpbbUwiykSJawFiU#z=9-iY-WFFGVZds9ktowtGKolj)

- 2019-05-01

  - Entity type
    Entity: Boss
    Entity type: [{eat, work},{report}]

    Entity: Developper
    Entity type: [{sleep, work},{blame}]

    Entity: Human
    Entity type: [{eat, work},{research}]

    Entity: Dog
    Entity type: [{eat, work, play},{feed}]

    Entity: Cat
    Entity type: [{eat, work, play},{feed}]

    Eat allowable entity type top 1: [{eat, work, play},{feed}]

- 2019-04-30

  - ![](https://imgur.com/tdoJIje.png)
  - FB15K-237
    - train 272115
    - valid 17535
    - test 20466
  - FB15K in OpenKE
    - train 483142
    - valid 50000
    - test 59071

- 2019-04-25

  - Question:
    - La fonction `getBestThreshold()` sert à quoi?
  - Rendez-vous avec Philippe
    - Verifier les papier Toutanova sur les problémes de corpus
    - Faire une version lisible pour FB15K-237
    - Essayer word2vec comme baseline sur FB15K-237

- 2019-04-23

  - lettre a prof
    Salut prof,

    Selon les expériences que j'ai fait l'année passée, j'ai mean rank 38 qui surpasse le single DsitMult in Kadlec et Al. 2017 (mean rank 42) sur FB15K, qui comprend 15K entities. Le baseline basé sur la fréquence a nous donnée mean rank 3449. J'ai pas trouvé les points intéressant. Est-ce que tu penses que il faut encore vérifier les détail? E.g: On regarde si les examples qui ne donnent pas très bien résultats dans DistMult/TransE peuvent donnent meilleure résultat dans baseline pour confirmer que DistMult/TransE portent déjà toutes des information de fréquence?

    Je essaye de comprendre plus sur le knowledge embeddings. Je trouve que la partie negative sampling est intéressante. Et je trouve que, dans word2vec, TransE, OpenKE, il semble qu'il utilise différentes méthodes de negative sampling. Selon Han et al. 2018, les triplets ont grande influence sur résultat final. Je veux prendre plus de temps pour comprendre cette partie.

    Ensuit, je trouve que les variants du TransE m'intéressent aussi. TransH, TransR, TransD, TransA, TransG, Transparse, ils servent à résoudre différents problème de TransE. Fondamentalement ils font la sommation de arg1 et relation. Si tu ne rejette pas, je vais mettre un peu de temps sur eux. Mais Ils sont aussi presque proposés par chinois! C'est vraiment bizarre.

    Du coup, la recherche de géométrie de knowledge graph embeddings m'intéresse aussi. Dans le travail de Chandrahas et al. 2018, ils ont montré le conicity (la moyenne de similarité de chaque vecteur par rapport à la moyenne des vecteurs) des models. Je crois que ce travail nous aidera à comparer les différences entre le model additive et celui multiplicative.

    Je vais commencer à faire la cross-validation pour confirmer mon meilleur résultat. J'espère que je peux obtenir un bon résultat dans deux semaines. 

    Quand tu seras disponible cette semaine? Nous pourrions en parler.

- 2019-04-20

  - outline for the email to Philippe:

    - baseline on FB15K

      - MR AVG = 3449.13735

         

        - MR arg1 = 3742.7826
        - MR arg2 = 3155.4921

      - Est-ce qu'il faut vérifier encore les détail?

      - DistMult: nbatches = 500, train_times = 100, Adam, lr= 0.001, n=1000,(max-margin + NLL + softmax as the loss, 5 mins train on GPU, MR=38 outperformed single distmult in Kadlec et Al. 2017 /MR= 42, MRR = 0.798/, our MRR= 0.535 is a little worse.)

        - il faut faire de la cross-validation

    - Negative Sampling

      - negative sampling in word2vec
        - [Part 2 Negative sampling](https://dynalist.io/d/pjG8fAbeOPV-j8Yr1b-i_FkU#z=kIVIn1KOI47ldnFeD9oFvMxr)
      - negative sampling in TransE
      - negative sampling in OpenKE

    - D'autres recherches

      - Les variants de TransE ( TransH, TransR, TransD, TransA, TransG, Transparse)
      - Lire plus de survey
      - 

  - [CBOW vs skip-gram](https://dynalist.io/d/pjG8fAbeOPV-j8Yr1b-i_FkU#z=cmG-O-0BErZDXzQZT6oOSU9M)

- 2019-04-19

  - negative sampling
    - in Han Xu's paper

- 2019-04-16

  - How does OpenKE do when the test entity has not been seen in training?
    - In file Test.h line 50-57
      - It shows it will return the max+1
  - Baseline on FB15K
    - MR AVG = 3449.13735
    - MR arg1 = 3742.7826
    - MR arg2 = 3155.4921

- 2019-04-01

  - Add f.flush into Configure.py
  - Succeeded xp069-2

- 2019-03-26

  - xp069-2-test at 09:14 run on malaga02 succeeded
  - Run xp069-2 at 21:38 on malaga02

- 2019-03-16

  - xp069-2-test(train times 1 instead of 500) Failed

- 2019-03-13

  - xp069-2 Failed

- 2019-03-11

  - xp042-2 Finished
  - xp069-2 Failed
  - Run xp069-2 at 11:13 on malaga02

- 2019-03-08

  - Found xp042-2 on ilar01 and xp069-2 on malaga01 **error**
  - Run xp042-2 at 18:41 on malaga02 
  - Run xp069-2 at 18:41 on malaga01

- 2019-03-06

  - Get Baseline V2+ cluster V4 OpenKE format log:

     

    - sim_freq_rank_2_cluster
    - baseline_v2_cluster_v4.log

- 2019-03-05

  - Run xp069-2 at 17:49 on malaga01

  - Get Baseline V2 OpenKE format log:

     

    - sim_freq_rank_2.ipynb
    - baseline_v2.log

- 2019-03-04

  - Run xp042-2 at 13:29 on ilar01 
  - Run xp069-2 at 21:32 on malaga01 **disconnected**

- 2019-03-03

  - Refer to this report
    - https://nbviewer.jupyter.org/github/quatrejuin/try_ke_models/blob/master/report/report.ipynb
    - Best TransE: xp069
    - Best TransE+cluster V2: xp042
    - Best Baseline V2:
    - Best Baseline V2+ cluster V4: 
  - Run xp069-2 at 23:30 on otctal03 **killed**

- 2019-02-27

  - je comprends déjà le "negative sampling" dans word2vec. D'après ce tutoriel ([http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)). Le "negative sampling" sert simplement à faciliter le calcul quand on met à jour des paramètres.
  - Je suis en train d'essayer de comprendre le "negative sampling" in OpenKE (Dans la fonction getBatch() du fichier base.cpp) #openke
